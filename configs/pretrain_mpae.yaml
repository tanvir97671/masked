# MPAE + SICR self-supervised pretraining config
defaults:
  base: "configs/base.yaml"

# Training - L40S optimized
training:
  max_epochs: 80
  batch_size: 512         # 2x increase for L40S 48GB
  lr: 2.0e-4              # Scaled 2x with batch size
  weight_decay: 1.0e-5
  warmup_epochs: 5
  scheduler: "cosine"
  gradient_clip_val: 1.0
  compile: true           # torch.compile for 2x speedup

# Data
data:
  fraction: 1.0             # use all data for SSL (set 0.25 for quick run)
  protocol: "pooled"
  include_unknown: true       # use 'unkn' class data for SSL too

# Encoder - L40S optimized (larger capacity)
encoder:
  type: "transformer"         # "transformer" or "cnn"
  d_model: 512                # 4x larger for full L40S utilization
  n_layers: 8                 # Deeper for better representations
  n_heads: 8
  ffn_dim: 2048               # 4x FFN expansion
  dropout: 0.1
  patch_size: 16
  gradient_checkpointing: true  # Save memory for larger models

# MPAE
mpae:
  mask_ratio: 0.6
  decoder_dim: 128       # Larger decoder
  decoder_layers: 4      # Deeper decoder

# SICR
sicr:
  lambda_sicr: 0.1
  temperature: 0.07
  projection_dim: 128    # Match encoder scale

# Logging
logging:
  log_every_n_steps: 50
  save_top_k: 2
  monitor: "val_loss"
