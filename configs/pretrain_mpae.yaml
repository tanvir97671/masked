# MPAE + SICR self-supervised pretraining config
defaults:
  base: "configs/base.yaml"

# Training - L4 (24GB) optimized
training:
  max_epochs: 80
  batch_size: 256           # Fits L4 24GB with bf16
  lr: 1.5e-4               # Scaled with batch size
  weight_decay: 1.0e-5
  warmup_epochs: 5
  scheduler: "cosine"
  gradient_clip_val: 1.0
  compile: true             # torch.compile for speedup

# Data
data:
  fraction: 1.0             # use all data for SSL (set 0.25 for quick run)
  protocol: "pooled"
  include_unknown: true       # use 'unkn' class data for SSL too

# Encoder - L4 optimized
encoder:
  type: "transformer"         # "transformer" or "cnn"
  d_model: 256                # Balanced for L4 24GB
  n_layers: 6                 # 6 layers is sufficient
  n_heads: 8
  ffn_dim: 1024               # 4x d_model expansion
  dropout: 0.1
  patch_size: 16
  gradient_checkpointing: true  # Save memory

# MPAE
mpae:
  mask_ratio: 0.6
  decoder_dim: 64          # Lightweight decoder
  decoder_layers: 2         # 2 layers sufficient

# SICR
sicr:
  lambda_sicr: 0.1
  temperature: 0.07
  projection_dim: 64       # Compact projection

# Logging
logging:
  log_every_n_steps: 50
  save_top_k: 2
  monitor: "val_loss"
